{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\kaasa\\Documents\\DriveBackup\\GitHub\\AI_Tutor\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "from generator import load_csm_1b\n",
    "import torchaudio\n",
    "import torch\n",
    "generator = load_csm_1b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "context1= \"\"\"Think of it like this imagine you're trying to describe a bunch of different objects to someone who's never seen them before you've got to use words that are unique to each object so they can understand what you mean. In computer vision we do something similar with images these are called feature vectors or descriptors because they help computers match an image to the ones it's seen many times before \"\"\"\n",
    "context2 = \"\"\"The codebook is like a list of these feature vectors it's used to build the embeddings which are basically vector representations of each image in the codebook, Embeddings are how we map objects to a high dimensional space where objects that are similar are close together and objects that are different are far apart think of it like a big graph with images as nodes and edges between nodes, when you look at the same object multiple times they'll form a connection to each other.\"\"\"\n",
    "audio = generator.generate(\n",
    "    text=context1,\n",
    "    speaker=1,\n",
    "    context=[],\n",
    "    max_audio_length_ms=100_000,\n",
    ")\n",
    "\n",
    "torchaudio.save(\"audio.wav\", audio.unsqueeze(0).cpu(), generator.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import Segment\n",
    "\n",
    "def load_prompt_audio(audio_path: str, target_sample_rate: int) -> torch.Tensor:\n",
    "    audio_tensor, sample_rate = torchaudio.load(audio_path)\n",
    "    audio_tensor = audio_tensor.squeeze(0)\n",
    "    # Resample is lazy so we can always call it\n",
    "    audio_tensor = torchaudio.functional.resample(\n",
    "        audio_tensor, orig_freq=sample_rate, new_freq=target_sample_rate\n",
    "    )\n",
    "    return audio_tensor\n",
    "\n",
    "def prepare_prompt(text: str, speaker: int, audio_path: str, sample_rate: int) -> Segment:\n",
    "    audio_tensor = load_prompt_audio(audio_path, sample_rate)\n",
    "    return Segment(text=text, speaker=speaker, audio=audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context1= \"\"\"Think of it like this imagine you're trying to describe a bunch of different objects to someone who's never seen them before you've got to use words that are unique to each object so they can understand what you mean. In computer vision we do something similar with images these are called feature vectors or descriptors because they help computers match an image to the ones it's seen many times before \"\"\"\n",
    "context2 = \"\"\"The codebook is like a list of these feature vectors it's used to build the embeddings which are basically vector representations of each image in the codebook, Embeddings are how we map objects to a high dimensional space where objects that are similar are close together and objects that are different are far apart think of it like a big graph with images as nodes and edges between nodes, when you look at the same object multiple times they'll form a connection to each other.\"\"\"\n",
    " \n",
    "SPEAKER_PROMPTS = {\n",
    "    \"conversational_a\": {\n",
    "        \"text\": context1,\n",
    "        \"audio\": \"audio1.wav\"\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt_a = prepare_prompt(\n",
    "        SPEAKER_PROMPTS[\"conversational_a\"][\"text\"],\n",
    "        1,\n",
    "        SPEAKER_PROMPTS[\"conversational_a\"][\"audio\"],\n",
    "        generator.sample_rate\n",
    "    )\n",
    "\n",
    "\n",
    "generated_segments = []\n",
    "prompt_segments = [prompt_a, ]\n",
    "\n",
    "conversation = [\n",
    "        {\"text\": context2, \"speaker_id\": 1},\n",
    "]\n",
    "partialcontext = prompt_segments + generated_segments\n",
    "\n",
    "for utterance in conversation:\n",
    "    print(f\"Generating: {utterance['text']}\")\n",
    "    audio_tensor = generator.generate(\n",
    "        text=utterance['text'],\n",
    "        speaker=utterance['speaker_id'],\n",
    "        context=partialcontext,\n",
    "        max_audio_length_ms=30_000,\n",
    "    )\n",
    "    generated_segments.append(Segment(text=utterance['text'], speaker=utterance['speaker_id'], audio=audio_tensor))\n",
    "    partialcontext.append(generated_segments[-1])\n",
    "    partialcontext = partialcontext[1:]\n",
    "\n",
    "# Concatenate all generations\n",
    "all_audio = torch.cat([seg.audio for seg in generated_segments], dim=0)\n",
    "torchaudio.save(\n",
    "    \"audio.wav\",\n",
    "    all_audio.unsqueeze(0).cpu(),\n",
    "    generator.sample_rate\n",
    ")\n",
    "print(\"Successfully generated full_conversation.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context1= \"\"\"Think of it like this imagine you're trying to describe a bunch of different objects to someone who's never seen them before you've got to use words that are unique to each object so they can understand what you mean. In computer vision we do something similar with images these are called feature vectors or descriptors because they help computers match an image to the ones it's seen many times before\n",
    "\"\"\"\n",
    "context2=\"\"\"The codebook is like a list of these feature vectors it's used to build the embeddings which are basically vector representations of each image in the codebook, Embeddings are how we map objects to a high dimensional space where objects that are similar are close together and objects that are different are far apart think of it like a big graph with images as nodes and edges between nodes, when you look at the same object multiple times they'll form a connection to each other.\"\"\",\n",
    "    \n",
    "SPEAKER_PROMPTS = {\n",
    "    \"conversational_a\": {\n",
    "        \"text\": context1,\n",
    "        \"audio\": \"audio1.wav\"\n",
    "    },\n",
    "    \"conversational_b\": {\n",
    "        \"text\": context2,\n",
    "        \"audio\": \"audio2.wav\"\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt_a = prepare_prompt(\n",
    "        SPEAKER_PROMPTS[\"conversational_a\"][\"text\"],\n",
    "        1,\n",
    "        SPEAKER_PROMPTS[\"conversational_a\"][\"audio\"],\n",
    "        generator.sample_rate\n",
    "    )\n",
    "prompt_b = prepare_prompt(\n",
    "        SPEAKER_PROMPTS[\"conversational_b\"][\"text\"],\n",
    "        1,\n",
    "        SPEAKER_PROMPTS[\"conversational_b\"][\"audio\"],\n",
    "        generator.sample_rate\n",
    "    )\n",
    "\n",
    "generated_segments = []\n",
    "prompt_segments = [prompt_a, prompt_b]\n",
    "\n",
    "conversation = [\n",
    "        {\"text\": \"Think of it like this imagine you're trying to describe a bunch of different objects to someone who's never seen them before you've got to use words that are unique to each object so they can understand what you mean, \", \"speaker_id\": 1},\n",
    "        {\"text\": \"In computer vision we do something similar with images these are called feature vectors or descriptors because they help computers match an image to the ones it's seen many times before, \", \"speaker_id\": 1},\n",
    "        {\"text\": \"The codebook is like a list of these feature vectors it's used to build the embeddings which are basically vector representations of each image in the codebook, \", \"speaker_id\":1},\n",
    "        {\"text\": \"Embeddings are how we map objects to a high dimensional space where objects that are similar are close together and objects that are different are far apart, \", \"speaker_id\": 1},\n",
    "        {\"text\": \"think of it like a big graph with images as nodes and edges between nodes when you look at the same object multiple times they'll form a connection to each other, \", \"speaker_id\": 1},\n",
    "        {\"text\": \"In practice embeddings are usually learned during training using some algorithm such as autoencoders or convolutional neural networks which are trained on large datasets of labeled images, \", \"speaker_id\": 1},\n",
    "        {\"text\": \"The idea is that by embedding all the images in a dataset we can create a kind of digital fingerprint for each object which allows computers to compare and match them with other objects even if they're not directly visible to humans.\", \"speaker_id\":1}\n",
    "    ]\n",
    "partialcontext = prompt_segments + generated_segments\n",
    "\n",
    "for utterance in conversation:\n",
    "    print(f\"Generating: {utterance['text']}\")\n",
    "    audio_tensor = generator.generate(\n",
    "        text=utterance['text'],\n",
    "        speaker=utterance['speaker_id'],\n",
    "        context=partialcontext,\n",
    "        max_audio_length_ms=25_000,\n",
    "    )\n",
    "    generated_segments.append(Segment(text=utterance['text'], speaker=utterance['speaker_id'], audio=audio_tensor))\n",
    "    #partialcontext.append(generated_segments[-1])\n",
    "    #partialcontext = partialcontext[1:]\n",
    "\n",
    "# Concatenate all generations\n",
    "all_audio = torch.cat([seg.audio for seg in generated_segments], dim=0)\n",
    "torchaudio.save(\n",
    "    \"full_conversation.wav\",\n",
    "    all_audio.unsqueeze(0).cpu(),\n",
    "    generator.sample_rate\n",
    ")\n",
    "print(\"Successfully generated full_conversation.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
